# 2021.10.11

[toc]

## *Forecast with Forecasts: Diversity Matters*

***key words*：Forecasting, Forecast Combination, Forecast Diversity, Prediction Intervals，Empirical Evaluation**

meta-learning，feature-based forecasting

>  In this work, we suggest a change of focus from the historical data to the produced forecasts to extract features. We use out-of-sample forecasts to obtain weights for forecast combinations by amplifying the diversity of the pool of methods being combined.

将重点从**历史数据的特征提取**转移到**生成预测进而提取特征**。

历史数据特征提取的缺点：

1. 历史数据的特征和未来数据的特征存在差异。
2. 数据有限时，某些特征的估计可能会不可行（not feasible）和不稳健（not robust）。
3. 数据包含large numbers时，可能会增加估计所需的时间。


$$
MES_{comb} = \frac{1}{H}\sum^H_{i = 1}(\sum^M_{i = 1}w_if_{ih} - y_{T+h})^2
$$

$$
<<<<<<< HEAD
= \sum^M_{i = 1}w_iMSE_i - \sum^{M-1}_{i=1}\sum^M_{j > i}w_iw_jDiv_{i, j}
=======
= \sum^M_{i = 1}w_iMSE_i - \sum^{M-1}_{i=1}\sum^M_{j > i}w_iw_jDiv_{i, j}\,
>>>>>>> d2c4b208450ac2b68ebe3a5c5643bd060304900e
$$

其中
$$
Div_{i, j} = \frac{1}{H}\sum^H_{i=1}(f_{ih} - f_{jh})^2
$$
表示组合预测的均方误差小于等于单个预测的加权均方误差。$ \sum^{M-1}_{i=1}\sum^M_{j > i}w_iw_jDiv_{i, j} $ 代表预测的多样性。说明预测方法池中存在的多样性越多，整体预测精度越高。

#### **FFORMA**(Feature-based forecast model averaging)

思路：从原始序列中提取特征若干，根据这些特征建立不同的元模型，再使用XGBoost算法以最小化总体加权平均误差为目标训练出权重。

缺点：

1. 需要手动选择特征。
2. 基于历史数据提取特征。



#### 文章思路

优化问题：
$$
\mathop{\arg\min}\limits_{\omega}\sum_{n=1}^N\sum_{i=1}^M\omega(Div_n)_i\times Err_{ni}
$$
其中$Div_n$代表第n个时间序列的预测多样性，$\omega(Div_n)_i$是对应第i个方法的权重，$Err_{ni}$是预测误差。 $\omega(Div_n)_i$​是XGBoost的输出经过softmax转换后的结果:
$$
\omega(Div_n)_i = \frac{exp\{y(Div_n)_i\}}{\sum^{M}_{i=1}exp\{y(Div_n)_i\}}
$$
$y(Div_n)_i$是XGBoost的输出结果。



思路优点：

1. 提取多样性的过程是直接的，具有可解释性。基于多样性的预测组合只需要来自单个方法的预测，避免了使用来自其他模型的信息。
2. 自动选取合适的特征集







## *A combination selection algorithm on forecasting*

**Shuang Cang, Hongnian Yu**

European Journal of Operational Research（2014）



***key words*：Neural networks, Seasonal autoregressive integrated moving average, Combination forecast, Information theory**

> This paper proposes an **optimal subset selection algorithm from all individual models** using **information theory**. The experimental results in tourism demand forecasting demonstrate that the combination of the individual models from the **selected optimal subset significantly outperforms** the combination of all available individual models. 

### Mutual imformation

#### Discrete

The $MI$ between two **discrete** random vector variables $U$​​​​ and $V$​​​​ :
$$
MI(U, V) = \sum_{u \in U}\sum_{v \in V}p(u, v)\text{log}\frac{p(u, v)}{p(u)p(v)},
$$
where $p(u, v)$ is a joint density function and $p(u)$ is a marginal density function.

**Shannon Entropy**：
$$
H = - K\sum^n_{i=1}p_i\text{log}p_i\ \ \ \ \text{where\ K\ is\ a\ positive\ constant.}
$$

Using the **entropy concept**:
$$
MI(U, V) = H(U) + H(V) - H(U, V),
$$
where 
$$
H(U, V) = -\sum_{u, v}p_{u, v}\text{log}p_{u, v}\ \ \ \ \ \ 
H(U) = -\sum_{u}p_u\text{log}p_u.
$$


$H(U)$​​ is an entropy and a measure of the amount of **uncertainty** associated with the value of $X$​​. 

$H(U, V)$​is a joint entropy which measures how much entropy is contained in a joint system of two random vector variables($U$​ and $V$​​​).

#### Continuous

The *MI* between two continuous random vector variables $X$ and $Y$:
$$
MI(X, Y) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}p(X, Y)\text{log}\frac{p(X, Y)}{p(X)p(Y)}dXdY,
$$
where $p(X, Y)$​ is a joint density function, and $p(X)$​ and $p(Y)$​ are the marginal density functions.

Using the **entropy concept**:
$$
MI(X, Y) = H(X) + H(Y) - H(X, Y),
$$
where 
$$
H(X, Y) = -\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}p(X, Y)\text{log}p(X,Y)dXdY
$$


$$
H(X) = -\int_{-\infty}^{\infty}p(X)\text{log}p(X)dX.
$$

$H(X)$​​​is an **entropy** and a measure of the amount of **uncertainty** associated with the value of $X$​​. 

$H(X, Y)$​​ is a **joint entropy** which measures **how much entropy is contained** in a joint system of two random vector variables ($X$​​ and $Y$​​).

There are no analytical solutions for the terms $H(X) = \int_{-\infty}^{\infty}p(X)\text{log}p(X)dX$ and $H(X, Y) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}p(X, Y)\text{log}p(X,Y)dXdY$ in order to calculate $MI(X, Y)$​.

##### Approximations

Expectation:
$$
\int_{-\infty}^{\infty}p(X)\text{log}p(X)dX \approx \frac{1}{N}\sum^N_{i=1}\text{log}p(X^{(i)})
$$


$$
\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}p(X, Y)\text{log}p(X,Y)dXdY \approx \frac{1}{N}\text{log}p(X^{(i)}, y^{(i)}).
$$

Parzen's window with the multivariate Gaussian Kernal-based function:
$$
p(X^{(i)}) = \frac{1}{N}\sum^N_{k=1}\frac{1}{(2\pi\sigma^2)^{d/2}}exp(-\frac{||X^{(i)} - X^{(k)}||^2}{2\sigma^2}))
$$


$$
p(X^{(i)}, y^{(i)}) = \frac{1}{N}\sum^N_{k=1}\frac{1}{(2\pi\sigma^2)^{(d+1)/2}}exp(-\frac{||X^{(i)} - X^{(k)}||^2 + (y^{(i)} - y^{(k)})^2}{2\sigma^2})).
$$



### MI Algorithm for optimal subset selection:

The forward (backward) selection is applied.

The **order** of the individual models for the optimal subset is determined by the **MI algorithm** and the **size** of optimal subsets is determined by the **FEM** (forcasting error measurement).

Find the maximum $MI(S\bigcup F_j, y)$​​ until there is non-significant improvement of the FEM, where $S$​​ is the selected data, $F_j$​​ is the outputs (forecasting values) of the individual model, $y$​​ is the actual outputs.

**MAPE(The Mean Absolute Perventage Error)**:
$$
\frac{1}{N}\sum^N_{t=1}|\frac{y_t-\hat{y_t}}{y_t}|
$$





## *Ensemble learning via negative correlation*

***key words*：Neural network ensembles; Negative correlation learning; Generalisation; Correlation; Combination method; Correct response set**

> Negative correlation learning attempts to train individual networks in an ensemble and combines them in the same learning process.In negative correlation learning, all the individual networks in the ensemble are trained simultaneously and interactively through the correlation penalty terms in their error functions. 

